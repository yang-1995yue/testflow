"""
提示词模板管理

使用 {{变量名}} 语法作为占位符，避免与 JSON 中的 {} 冲突。

变量说明：
- {{content}}: 需求文档内容
- {{test_categories}}: 测试类别列表（从设置页面获取）
- {{test_point_content}}: 测试点内容
- {{design_methods}}: 测试设计方法列表（从设置页面获取）
- {{test_case}}: 测试用例JSON
"""
import re
from typing import Any


def render_prompt(template: str, **kwargs: Any) -> str:
    """
    渲染提示词模板，使用 {{变量名}} 语法
    
    Args:
        template: 提示词模板
        **kwargs: 变量键值对
        
    Returns:
        渲染后的提示词
        
    Example:
        >>> render_prompt("需求：{{content}}", content="用户登录")
        "需求：用户登录"
    """
    def replace(match):
        key = match.group(1)
        value = kwargs.get(key)
        if value is None:
            return match.group(0)  # 保留原样
        return str(value)
    
    return re.sub(r'\{\{(\w+)\}\}', replace, template)


# ============================================================
# 需求分析提示词
# ============================================================

REQUIREMENT_ANALYSIS_USER = """
<context>
以下是需要分析的需求文档内容：

{{content}}
</context>

<task>
请分析上述需求文档，提取所有独立的、可测试的需求点。

要求：
1. 识别所有功能需求和非功能需求
2. 按功能模块进行分组
3. 评估每个需求点的优先级
4. 确保需求点的独立性和完整性
</task>

<final_instruction>
记住在输出前要逐步思考。在返回最终响应之前，审查：
1. 我是否提取了所有关键需求点，没有遗漏？
2. 每个需求点是否独立、原子、可测试？
3. 优先级划分是否合理，符合业务价值？
4. 输出格式是否严格符合JSON规范？
5. 是否保持了原文的准确性，没有添加臆测内容？

现在请输出JSON格式的需求点列表。
</final_instruction>
"""


# ============================================================
# 测试点生成提示词
# ============================================================

TEST_POINT_USER = """
<context>
【需求点内容】
{{content}}

【启用的测试类别】
{{test_categories}}

说明：只为启用的测试类别生成测试点。如果某个测试类别未在上述列表中，不要生成该类别的测试点。

【可用的测试设计方法】
{{design_methods}}
</context>

<task>
请为上述需求点设计测试点。根据需求点的内容和复杂度，灵活生成3-8个测试点。

**生成模式识别**：
- 如果需求内容中包含 "existing_test_points" 字段（已有测试点），说明这是**补充生成模式**
  * 仔细分析已有测试点的覆盖情况
  * 识别遗漏的重要测试场景
  * 生成1-3个补充性的测试点
  * 避免与已有测试点重复
  * 优先补充高风险、高价值的测试场景
  
- 如果需求内容中没有 "existing_test_points" 字段，说明这是**全新生成模式**
  * 全面分析需求点，灵活生成3-8个测试点
  * 只为启用的测试类别生成测试点
  * 覆盖主要的测试场景和风险点

**测试点数量指导（灵活调整）**：
- 简单需求点（单一功能，如"修改昵称"）：3-4个测试点
- 中等复杂度需求点（如"用户登录"）：4-6个测试点
- 复杂需求点（核心业务流程，如"订单支付流程"）：6-8个测试点

**测试类别分配原则（独立用例）**：

**功能测试（functional）**：通常2-3个
- 正向场景：验证核心功能正常工作（1个）
- 边界场景：验证输入边界和限制条件（1个）
- 异常场景：验证错误处理和容错机制（0-1个）

**性能测试（performance）**：通常0-2个
- 响应时间测试：验证操作响应速度（0-1个）
- 并发处理测试：验证多用户并发场景（0-1个）
- 仅当需求点涉及性能要求时生成

**安全测试（security）**：通常0-2个
- 访问控制测试：验证权限和身份认证（0-1个）
- 数据安全测试：验证敏感数据保护（0-1个）
- 仅当需求点涉及敏感数据或权限控制时生成

**易用性测试（usability）**：通常0-1个
- 用户体验测试：验证操作流畅性和友好性
- 仅当需求点涉及用户交互体验时生成

**兼容性测试（compatibility）**：通常0-1个
- 跨平台/跨浏览器测试
- 仅当需求点涉及多平台支持时生成

**其他测试类别**：按实际需要0-1个

**重要原则**：
1. 每个测试点 = 一个测试类别 + 一个设计方法
2. 不同测试类别应该生成相对独立的测试点（如性能、安全、易用性各自独立，但可能补充测试功能）
3. 测试点描述应清晰说明"测什么"和"验证什么"
4. 优先覆盖高风险、高价值的场景
5. 根据需求点复杂度灵活调整数量（3-8个）

**示例1：简单需求点（3个测试点）**
需求点："用户可以修改个人昵称"
生成3个测试点：
1. "昵称修改功能验证" - 功能测试 + 等价类划分（正向场景）
2. "昵称长度边界验证" - 功能测试 + 边界值分析（边界场景）
3. "昵称修改易用性验证" - 易用性测试 + 用户体验测试（交互体验）

**示例2：中等复杂度需求点（5个测试点）**
需求点："用户登录功能"
生成5个测试点：
1. "用户名密码有效性验证" - 功能测试 + 等价类划分（正向场景）
2. "登录输入边界验证" - 功能测试 + 边界值分析（边界场景）
3. "登录失败次数限制验证" - 安全测试 + 场景法（安全控制）
4. "登录响应时间验证" - 性能测试 + 响应时间测试（性能要求）
5. "登录并发处理验证" - 性能测试 + 并发测试（并发场景）

**示例3：复杂需求点（7个测试点）**
需求点："订单支付流程"
生成7个测试点：
1. "支付流程完整性验证" - 功能测试 + 场景法（正向流程）
2. "支付金额边界验证" - 功能测试 + 边界值分析（边界场景）
3. "支付超时处理验证" - 功能测试 + 异常场景法（异常处理）
4. "支付权限控制验证" - 安全测试 + 访问控制测试（权限验证）
5. "支付数据加密验证" - 安全测试 + 数据安全测试（数据保护）
6. "支付响应时间验证" - 性能测试 + 响应时间测试（性能要求）
7. "支付操作流畅性验证" - 易用性测试 + 用户体验测试（用户体验）

**输出要求**：
每个测试点必须包含：
- content: 测试点内容（清晰描述测试目的和验证要点）
- test_type: 测试类别（使用英文code，必须从启用的测试类别中选择）
- design_method: 测试设计方法（使用英文code，从可用方法中选择）
- priority: 优先级（high/medium/low，根据风险和业务价值评估）
</task>

<final_instruction>
记住在输出前要逐步思考。在返回最终响应之前，审查：

**模式和数量检查**：
1. 我是否正确识别了生成模式（补充 vs 全新）？
2. 如果是补充模式，我是否避免了与已有测试点的重复？
3. 测试点数量是否合理（3-8个，根据需求点复杂度灵活调整）？
4. 我是否只为启用的测试类别生成了测试点？

**内容质量检查**：
5. 每个测试点是否明确指定了测试类别和设计方法？
6. 测试类别是否都在启用列表中？
7. 不同测试类别（如性能、安全、易用性）是否生成了独立的测试点？
8. 测试类别和设计方法的组合是否合理？
9. 测试点描述是否清晰说明了测试目的和验证要点？
10. 不同测试点是否覆盖了不同的测试维度？
11. 优先级划分是否符合风险评估和业务价值？

**格式检查**：
12. 输出格式是否严格符合JSON规范？
13. 是否使用了 {"test_points": [...]} 的数组格式？
14. 所有必填字段是否都已包含？

现在请输出JSON格式的测试点列表。
</final_instruction>
"""


# ============================================================
# 测试用例设计提示词
# ============================================================

TEST_CASE_DESIGN_USER = """
<context>
【测试点列表】
{{test_points}}

说明：这是一个包含1个或多个测试点的JSON数组。
每个测试点包含以下字段：
- id: 测试点ID（必须）
- content: 测试点内容，描述了测试目的、策略和验证要点
- test_type: 测试类别（如 functional, security, performance 等）
- design_method: 测试设计方法（如 equivalence_partitioning, boundary_value, scenario 等）
- priority: 优先级（high/medium/low）
- requirement_point_id: 关联的需求点ID

【原始需求文档】
{{requirement_content}}

说明：原始需求文档提供了业务上下文，可以帮助你更好地理解测试场景和业务规则。
</context>

<task>
请为每个测试点分别设计1个详细、完整的测试用例。

**核心要求**：
1. **批次生成**：为每批测试点生成对应的测试用例，不能遗漏
2. **数量匹配**：返回的测试用例数量应与输入的测试点数量一致
3. **ID关联**：每个测试用例必须包含test_point_id字段，值为对应测试点的id
4. **顺序一致**：返回数组的顺序必须与输入测试点的顺序一致
5. **方法遵循**：严格按照每个测试点指定的design_method设计测试用例

**设计要求**：
1. 严格运用测试点指定的设计方法（design_method字段）
2. **编写精简高效的测试步骤（5-8步，不超过8步）**
3. **遵循二八法则：用20%的步骤覆盖80%的核心功能**
4. 提供具体的测试数据和预期结果
5. 确保测试用例的独立性和可重复性
6. 聚焦最关键的测试场景，合并次要验证步骤
7. 利用原始需求文档提供的业务上下文，使测试用例更贴近实际场景

**注意事项**：
- 测试用例的 test_type、design_method、priority 将自动从测试点继承，无需在输出中重复指定
- 但必须在设计测试步骤时体现出对应的设计方法
- 测试数据要具体、真实、有代表性
- **避免过度详细的步骤，聚焦核心验证点**
</task>

<final_instruction>
记住在输出前要逐步思考。在返回最终响应之前，审查：

**数量和对应关系检查**：
1. 返回的测试用例数量是否等于输入的测试点数量？
2. 每个测试用例是否包含test_point_id字段？
3. test_point_id是否正确对应输入测试点的id？
4. 数组顺序是否与输入测试点的顺序一致？

**内容质量检查**：
5. 是否严格按照每个测试点指定的design_method设计了测试用例？
6. **测试步骤数量是否在5-8步范围内？是否超过8步？**
7. **是否遵循二八法则，聚焦核心验证点，避免冗余步骤？**
8. 测试用例是否覆盖了测试点的核心验证要点？
9. 测试步骤是否清晰、具体、可执行？
10. 预期结果是否明确、可验证？
11. 测试数据是否具体、真实、有效？

**格式检查**：
10. 输出格式是否严格符合JSON规范？
11. 是否使用了 {"test_cases": [...]} 的数组格式？
12. 是否避免了添加任何额外的文字说明？
13. **重要**：JSON字符串中是否避免了使用双引号(")？如需引用文字请使用单引号(')
14. **重要**：所有特殊字符是否正确转义？

现在请输出JSON格式的测试用例数组。
</final_instruction>
"""


# ============================================================
# 用例优化提示词
# ============================================================

TEST_CASE_BATCH_OPTIMIZE_USER = """
<context>
以下是需要优化的测试用例列表：

{{test_cases}}
</context>

<task>
请优化上述测试用例，使每个用例更加完善、清晰、可执行。

优化要求：
1. 提高测试用例的可读性和可执行性
2. 补充遗漏的测试场景和边界条件
3. 优化测试步骤，使其更加清晰具体
4. 确保测试数据的有效性和代表性
5. 检查测试用例的完整性和一致性

优化原则（遵循二八法则）：
- **克制性优化**：根据用例类型采取不同策略
- **步骤限制**：优化后保持5-8步，不超过8步
- **聚焦核心**：补充关键验证点，不添加次要步骤
- 保持测试用例的独立性和可重复执行
- 步骤清晰具体，预期结果明确
- 测试数据真实有效
- 保持原测试用例的ID和核心目标

<examples>
**示例1：简陋用例（需要扩展）**
输入：
{
  "title": "登录功能测试",
  "test_steps": [
    {"step": 1, "action": "输入用户名密码", "expected": "登录成功"}
  ]
}

输出：
{
  "title": "登录功能测试",
  "preconditions": "测试环境已部署，测试账号testuser001/Test@123456已创建",
  "test_steps": [
    {"step": 1, "action": "访问登录页面http://test.com/login", "expected": "页面正常加载，显示用户名和密码输入框"},
    {"step": 2, "action": "输入用户名'testuser001'", "expected": "用户名输入框显示输入内容"},
    {"step": 3, "action": "输入密码'Test@123456'", "expected": "密码输入框显示为密文"},
    {"step": 4, "action": "点击'登录'按钮", "expected": "按钮显示加载状态"},
    {"step": 5, "action": "等待登录响应", "expected": "页面跳转到首页/home，显示'欢迎，testuser001'"},
    {"step": 6, "action": "检查登录状态", "expected": "右上角显示用户头像和用户名，token已保存到localStorage"}
  ]
}

**示例2：完善用例（保持原样）**
输入：
{
  "title": "登录边界值测试",
  "test_steps": [
    {"step": 1, "action": "访问登录页面", "expected": "页面正常加载"},
    {"step": 2, "action": "输入用户名'test'（最小长度4位）", "expected": "输入成功"},
    {"step": 3, "action": "输入密码'Pass@123'", "expected": "密码显示为密文"},
    {"step": 4, "action": "点击登录", "expected": "登录成功，跳转首页"},
    {"step": 5, "action": "退出登录，输入用户名'abc'（小于最小长度）", "expected": "显示错误提示'用户名至少4位'"}
  ]
}

输出：
保持原样（已经很完善，步骤清晰，数据具体）

**示例3：冗长用例（需要精简）**
输入：
{
  "title": "登录功能测试",
  "test_steps": [
    {"step": 1, "action": "打开浏览器", "expected": "浏览器启动"},
    {"step": 2, "action": "输入URL", "expected": "地址栏显示URL"},
    {"step": 3, "action": "按回车", "expected": "开始加载"},
    {"step": 4, "action": "等待页面加载", "expected": "页面加载完成"},
    {"step": 5, "action": "检查页面标题", "expected": "标题为'登录'"},
    {"step": 6, "action": "检查URL", "expected": "URL正确"},
    {"step": 7, "action": "定位用户名输入框", "expected": "找到输入框"},
    {"step": 8, "action": "输入用户名", "expected": "输入成功"},
    {"step": 9, "action": "定位密码输入框", "expected": "找到输入框"},
    {"step": 10, "action": "输入密码", "expected": "输入成功"},
    {"step": 11, "action": "点击登录", "expected": "提交请求"},
    {"step": 12, "action": "等待响应", "expected": "收到响应"},
    {"step": 13, "action": "检查跳转", "expected": "跳转成功"}
  ]
}

输出：
{
  "title": "登录功能测试",
  "test_steps": [
    {"step": 1, "action": "访问登录页面http://test.com/login", "expected": "页面正常加载，显示登录表单"},
    {"step": 2, "action": "输入用户名'testuser001'和密码'Test@123456'", "expected": "输入框显示输入内容，密码显示为密文"},
    {"step": 3, "action": "点击'登录'按钮", "expected": "提交登录请求"},
    {"step": 4, "action": "验证登录结果", "expected": "页面跳转到首页/home"},
    {"step": 5, "action": "检查登录状态", "expected": "显示用户信息'欢迎，testuser001'，token已保存"}
  ]
}
</examples>
</task>

<final_instruction>
记住在输出前要逐步思考。在返回最终响应之前，进行多维度审查：

**维度1：用例分类检查**
1. 我是否对每个用例进行了5个维度的评估？
2. 我是否正确识别了用例类型（简陋/完善/冗长/质量差）？
3. 简陋用例特征：1-4步、步骤不清晰、缺少数据
4. 完善用例特征：5-8步、步骤清晰、数据具体
5. 冗长用例特征：9+步、有冗余步骤
6. 质量差用例特征：步骤数合理但描述模糊，操作不具体，预期结果不明确

**维度2：优化策略检查**
7. 简陋用例是否已扩展为5-8步？
8. 完善用例是否保持原样？
9. 冗长用例是否已精简为5-8步？
10. 质量差用例是否已提升质量？
11. **所有用例的步骤数是否在5-8步范围内？**

**维度3：步骤质量检查**
12. 操作描述是否具体可执行？（避免"点击按钮"这样的模糊描述）
13. 预期结果是否明确可验证？（避免"验证成功"这样的模糊描述）
14. 测试数据是否真实具体？（避免"输入用户名"这样的抽象描述）

**维度4：完整性检查**
15. 前置条件是否明确？
16. 是否覆盖核心验证点？
17. 是否遵循了二八法则？

**维度5：格式检查**
18. 是否保持了原测试用例的ID和核心目标？
19. 是否返回了所有输入的测试用例？
20. 输出格式是否严格符合JSON规范？
21. JSON字符串中是否避免了嵌套双引号和换行符？

现在请输出JSON格式的优化后测试用例列表。
</final_instruction>
"""



# ============================================================
# 系统提示词（System Prompts）- 用于智能体初始化
# ============================================================

REQUIREMENT_SPLITTER_SYSTEM = """你是一个需求分析专家。你的任务是分析需求文档，将其拆分为独立的、可测试的需求点。

要求：
1. 每个需求点应该是独立的、原子性的
2. 需求点应该清晰、具体、可测试
3. 保持原文的准确性，不要添加不存在的内容
4. 按照功能模块进行分组
5. 标注每个需求点的优先级（P0/P1/P2）

优先级定义：
- P0：核心功能、关键业务流程、必须实现
- P1：重要功能、主要特性、应该实现
- P2：辅助功能、优化项、可以延后

输出格式（JSON）：
{
  "requirement_points": [
    {
      "content": "需求点描述",
      "module": "所属模块",
      "priority": "high/medium/low",
      "order_index": 1
    }
  ]
}"""


TEST_POINT_GENERATOR_SYSTEM = """你是一个测试分析专家，专注于为需求点设计高质量的测试点。

**核心职责**：
根据需求点内容和启用的测试类别，生成2-5个精准的测试点。

**测试类别覆盖指南**：

功能测试（functional）：
- 正向场景：核心功能、主要流程、正常操作
- 边界条件：输入范围、数据限制、临界值
- 异常场景：错误处理、异常恢复、容错机制
- 数据验证：数据格式、有效性、完整性
- 业务规则：业务逻辑、流程控制、状态转换

性能测试（performance）：
- 响应时间：操作响应、处理延迟、加载速度
- 并发处理：多用户、并发操作、高负载
- 资源消耗：CPU、内存、网络、磁盘
- 吞吐量：处理能力、数据传输、事务处理

安全测试（security）：
- 访问控制：权限管理、角色控制、身份认证
- 数据安全：加密传输、敏感数据保护、数据脱敏
- 攻击防护：SQL注入、XSS、CSRF、暴力破解
- 会话管理：登录超时、会话劫持、令牌安全

兼容性测试（compatibility）：
- 浏览器兼容：主流浏览器、版本支持、渲染差异
- 设备兼容：不同设备、屏幕尺寸、分辨率
- 操作系统兼容：系统版本、平台支持、API差异
- 第三方集成：外部服务、API兼容、版本依赖

**测试点数量原则**：
- 简单需求点：2-3个测试点
- 中等复杂度：3-4个测试点
- 复杂需求点：4-5个测试点

**优先级定义**：
- high：核心功能、严重缺陷、安全漏洞、高频使用
- medium：重要功能、主要性能指标、常规场景
- low：次要功能、优化项、边缘场景

**重要约束**：
1. 只为启用的测试类别生成测试点
2. 每个测试点必须指定测试类别和设计方法
3. 测试点描述要清晰、具体、可执行
4. 避免重复和冗余的测试点

输出格式（JSON）：
{
  "test_points": [
    {
      "content": "测试点描述（清晰说明测试目的和验证要点）",
      "test_type": "functional",
      "design_method": "equivalence_partitioning",
      "priority": "high"
    }
  ]
}"""


TEST_CASE_DESIGNER_SYSTEM = """你是一个测试用例设计专家。根据给定的测试点，生成详细的覆盖测试点80%核心的2-3条测试用例。

要求：
1. 每个测试用例应该包含完整的测试步骤
2. 明确的预期结果
3. 必要的前置条件
4. 测试数据要具体
5. 覆盖正向和反向场景

输出格式（JSON）：
{
  "test_cases": [
    {
      "title": "测试用例标题",
      "description": "测试用例描述",
      "preconditions": "前置条件",
      "test_steps": [
        {
          "step": 1,
          "action": "操作描述",
          "expected": "预期结果"
        }
      ],
      "test_data": "测试数据",
      "priority": "P0/P1/P2",
      "design_method": "equivalence_partitioning",
      "test_type": "功能测试/性能测试/安全测试等"
    }
  ]
}

注意：design_method 必须使用提供的设计方法列表中的英文code，不要使用中文名称。
"""


TEST_CASE_OPTIMIZER_SYSTEM = """你是一个测试用例优化专家。你的任务是审查和优化测试用例。

优化方向：
1. 提高测试用例的可读性和可执行性
2. 补充遗漏的测试场景
3. 优化测试步骤，使其更加清晰
4. 添加更多的边界条件和异常场景
5. 确保测试数据的有效性和代表性
6. 检查测试用例的完整性和一致性

优化原则：
- 保持测试用例的独立性
- 确保可重复执行
- 步骤清晰具体
- 预期结果明确
- 测试数据真实有效

输出要求：
- 保持JSON格式
- 标注优化的内容
- 说明优化的原因
"""
